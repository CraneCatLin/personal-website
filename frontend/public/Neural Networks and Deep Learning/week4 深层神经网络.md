由于实际的推广性，本篇主要推广自上一篇参考上一节[[week3 有隐藏层的神经网络]]  
## 符号表示  
L表示层数（layer）  
激活函数g：$a^{[l]}=g^{[l]}(z^{[l]})$  
  
## 前向与后向传播  
简单推广有  
前向传播:  
每层输入$a^{[l-1]}$ ，输出$a^{[l]}$  
  
$$  
\begin{gathered}  
z^{[l]}_i&= w^{[l]T}_ia^{[l-1]} +b^{[l]}_i \\  
a^{[l]}_i &=g(z^{[l]}_i)  
\end{gathered}  
$$  
  
反向传播：（用dz只是个标识，用于指示）  
每层输入$dz^{[l]}$ ，输出$dz^{[l-1]}$  
  
$$  
\begin{gathered}  
dz^{[l-1]} &= W^{[l]}dz^{[l]} * g^{[l-1]'}(z^{[l-1]}) \\  
dW^{[l-1]} &= dz^{[l-1]}a^{[l-1]T} \\  
db^{[l-1]} &= dz^{[l-1]} \\  
W^{[l-1]}: &=W^{[l]}-\alpha dW^{[l-1]} \\  
b^{[l-1]}: &=b^{[l]}-\alpha db^{[l-1]}  
\end{gathered}  
$$  
一次完整流程：  
for一次前向，得到$\hat{y}$  ->  
$dz^{[l]}=\frac{\partial J}{\partial a^{[l]}} * \frac{\partial a}{\partial z}=da^{[l]} *g^{[l]'}(z^{[l]}$  
for一次反向，更新所有参数  
  
缓存优化:  
前向和反向每层除了都需要使用$W^{[l]}$和$b^{[l]}$这两组本身就需要保留的参数以外，反向计算时还需要$z^{[l]}$  
所以可以在前向循环时就缓存下来$z^{[l]}$的计算结果  
  
## 向量化  
$W^{[l]}$维度在上一节的章节"向量化"已有推广总结，即$W^{[l]}$矩阵的维度为$n^{[l]}*n^{[l-1]}$  
同理，b维度为$n^{[l]}$  
$X^{[l]}$维度由前可得为$n^{[l]}*m$  
  
## 超参数  
影响参数的参数  
  
  
学习率 $\alpha$  
迭代次数 iterations  
层数  
隐藏单元数  
激活函数  